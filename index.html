<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Multi-Image Visual Question Answering for Unsupervised Anomaly Detection</title>
  <link rel="icon" type="image/x-icon" href="static/images/doraemeng.JPG">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Multi-Image Visual Question Answering for Unsupervised Anomaly Detection</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <!-- <a href="FIRST AUTHOR PERSONAL LINK" target="_blank">First Author</a><sup>*</sup>,</span>
                <span class="author-block">
                  <a href="SECOND AUTHOR PERSONAL LINK" target="_blank">Second Author</a><sup>*</sup>,</span>
                  <span class="author-block">
                    <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Third Author</a> -->
                    <a href="https://lijunrio.github.io/junli/" target="_blank">Jun Li</a><sup>1,2</sup>,
                    <span class="author-block">
                      <a href="https://cosmin-bercea.com/" target="_blank">Cosmin I. Bercea</a><sup>1,3</sup>,
                    </span>
                    <span class="author-block">
                      <a href="https://aim-lab.io/author/philip-muller/" target="_blank">Philip Müller</a><sup>1</sup>,
                    </span>
                    <span class="author-block">
                      <a href="https://compai-lab.github.io/author/lina-felsner/" target="_blank">Lina Felsner</a><sup>1</sup>,
                    </span>
                    <span class="author-block">
                      <a href="https://scholar.google.de/citations?user=2yp4OFgAAAAJ&hl=en" target="_blank">Suhwan Kim</a><sup>4</sup>,
                    </span>
                    <br>
                    <a href="https://aim-lab.io/author/daniel-ruckert/" target="_blank">Daniel Rueckert</a><sup>1,2,5</sup>,
                    <span class="author-block">
                      <a href="https://www.neurokopfzentrum.med.tum.de/neuroradiologie/mitarbeiter-profil-wiestler.html" target="_blank">Benedikt Wiestler</a><sup>4</sup>,
                    </span>
                    <span class="author-block">
                      <a href="https://compai-lab.github.io/author/julia-a.-schnabel/" target="_blank">Julia A. Schnabel</a><sup>1,2,3,6</sup>
                    </span>
                    
                  </span>
                  </div>

            <div class="is-size-5 publication-authors">
              <div style="display: flex; justify-content: space-between;">
                  <div style="flex: 1;">
                      <span class="author-block"><sup>1</sup> Technical University of Munich, Germany</span>
                      <span class="author-block"><sup>2</sup> Munich Center for Machine Learning, Germany</span>
                      <span class="author-block"><sup>3</sup> Helmholtz AI & Helmholtz Center Munich, Germany</span>
                  </div>
                  <div style="flex: 1;">
                      <span class="author-block"><sup>4</sup> Klinikum Rechts der Isar, Munich, Germany</span>
                      <span class="author-block"><sup>5</sup> Imperial College London, London, UK</span>
                      <span class="author-block"><sup>6</sup> King’s College London, London, UK</span>
                  </div>
              </div>
          </div>
            
            

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/2404.07622.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- Supplementary PDF link
                    <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span> -->

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/LijunRio/Multi-Image-VQA-for-UAD" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2404.07622" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="static/images/teaser.gif" alt="Teaser GIF" height="100%">
      <h2 class="subtitle has-text-centered">
        Our framework is designed to process questions in conjunction with results from anomaly detection methods aiming to provide clinicians with clear, interpretable responses that render anomaly map analyses more intuitive and clinically actionable.  
      </h2>
    </div>
  </div>
</section>


<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Unsupervised anomaly detection enables the identification of potential pathological areas by juxtaposing original images with their pseudo-healthy reconstructions generated by models trained exclusively on normal images. However, the clinical interpretation of resultant anomaly maps presents a challenge due to a lack of detailed, understandable explanations. Recent advancements in language models have shown the capability of mimicking human-like understanding and providing detailed descriptions. This raises an interesting question: <span style="font-style: italic;">How can language models be employed to make the anomaly maps more explainable?</span> To the best of our knowledge, we are the first to leverage a language model for unsupervised anomaly detection, for which we construct a dataset with different questions and answers. Additionally, we present a novel multi-image visual question answering framework tailored for anomaly detection, incorporating diverse feature fusion strategies to enhance visual knowledge extraction. Our experiments reveal that the framework, augmented by our new Knowledge Q-Former module, adeptly answers questions on the anomaly detection dataset. Besides, integrating anomaly maps as inputs distinctly aids in improving the detection of unseen pathologies. 
            
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- End paper abstract -->



<section class="section hero is-small">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full">
        <div class="content">
         <h2 class="title is-3">Methods</h2>
          <div class="level-set has-text-justified">
            <p>
              In this paper, we developed, to the best of our knowledge, the first multi-image question answering benchmark based on unsupervised anomaly detection. Our framework designs different feature fusion strategies for combing the anomaly map, original image, and PH reconstruction. Besides, inspired by the Querying Transformer (Q-Former), we propose a Knowledge Q-Former (KQ-Former) module to assist the framework in extracting visual features related to textual knowledge. Extensive experiments have been conducted to verify the effectiveness of the framework and proposed KQ-Former module. Additionally, we explore the influence of the anomaly map for the framework in facing unknown anomalies.
          </p>
        </p>
        </div><img src="static/images/Method8.jpg" alt="Dataset compression scale" class="blend-img-background center-image" style="max-width: 100%; height: auto; display: block; margin: auto;">
        </p>
        <div style="text-align: center;">
          <em>An overview of our proposed frameworks. (a) the multi-image VQA framework; (b) multi-image feature fusion strategies; (c) knowledge Q-former framework.</em>
      </div>
        </div>
        
      </div>
    </div>
  </div>

</section>


<section class="section hero is-small is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full">
        <div class="content">
         <h2 class="title is-3">Dataset</h2>
          <div class="level-set has-text-justified">
            <p>
              We collected 440 T1 weighted MRI 2D mid-axial brain images from the 
              <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8983757/" target="_blank" style="color: rgb(109, 109, 115); text-decoration: underline;">fastMRI dataset</a>, containing healthy (N=253) and unhealthy (N=187) data. The dataset features 8 distinct types of anomalies for the primary experiment and an additional 17 cases across 6 different pathologies to assess the model's generalization capabilities to unseen pathological conditions.
          </p>
        </p>
        </div><img src="static/images/Dataset8.jpg" alt="Dataset compression scale" class="blend-img-background center-image" style="max-width: 100%; height: auto; display: block; margin: auto;">
      </p>
      <div style="text-align: center;">
        <em>Overview of the anomaly dataset. Left: category distribution of anomalies. Right: definition of closed and open questions. For closed questions: blue text shows the answer type, number in parentheses denotes the count of answer types.</em>
    </div>  
      </div><img src="static/images/unseen_anomalies.jpg" alt="Dataset compression scale" class="blend-img-background center-image" style="max-width: 100%; height: auto; display: block; margin: auto;">
        </p>
        <div style="text-align: center;">
          <em>Category distribution of unseen anomalies. These unseen anomalies are dural thickening, white matter lesion, sinus opacification, encephalomalacia, intraventricular substance, and absent septum pellucidum.</em>
      </div>
        </div>
        
      </div>
    </div>
  </div>

</section>


<section class="section hero is-small">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full">
        <div class="content">
         <h2 class="title is-3">Results</h2>
          <div class="level-set has-text-justified">
            <p>
              In the main paper, we have evaluated our methods from three aspects: (1) Comparison between our Multi-Image VQA (MI) and KQ-Former (KQF) frameworks. (2) Benefits of Anomaly Maps. (3) Performance on Unseen Anomalies.
          </p>
          <div class="container">
            <div id="results-carousel" class="carousel results-carousel">
             <div class="item">
              <!-- Your image here -->
              <img src="static/images/table1.png" alt="MY ALT TEXT"/>
              <h2 class="subtitle has-text-centered">
                <!-- First image description. -->
              </h2>
            </div>
            <div class="item">
              <!-- Your image here -->
              <img src="static/images/table2.png" alt="MY ALT TEXT"/>
              <h2 class="subtitle has-text-centered">
                <!-- Second image description. -->
              </h2>
            </div>
            <div class="item">
              <!-- Your image here -->
              <img src="static/images/table3.png" alt="MY ALT TEXT"/>
              <h2 class="subtitle has-text-centered">
               <!-- Third image description. -->
             </h2>
           </div>
        </div>
      
        <p>
          We also utiliz Natural Language Inference (NLI) models to evaluate the entailment between ground truth and predicted sentence.
        </div><img src="static/images/table4.png" alt="Dataset compression scale" class="blend-img-background center-image" style="max-width: 100%; height: auto; display: block; margin: auto;">

      
      </p>
      <div style="text-align: center;">
      </p>
      </div>   
      </div>
    </div>
  </div>

</section>


<section class="section hero is-small is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full">
        <div class="content">
         <h2 class="title is-3">GUI Interface</h2>
          <div class="level-set has-text-justified">
            <p>

        </div>
        <p></p>
        <video poster="" id="tree" autoplay controls muted loop height="100%">
          <!-- Your video here -->
          <source src="static/videos/test_video2.mp4"
          type="video/mp4">
        </video>
      </div>   
      </div>
    </div>
  </div>

</section>



<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@misc{li2024multiimage,
        title={Multi-Image Visual Question Answering for Unsupervised Anomaly Detection}, 
        author={Jun Li and Cosmin I. Bercea and Philip Müller and Lina Felsner and Suhwan Kim and Daniel Rueckert and Benedikt Wiestler and Julia A. Schnabel},
        year={2024},
        eprint={2404.07622},
        archivePrefix={arXiv},
        primaryClass={cs.CV}
  }</code></pre>
    </div>
</section>




<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
